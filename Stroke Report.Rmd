---
title: "Stroke Prediction"
author: "Ray Sherbourne"
date: "4/2/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Overview

Strokes account for 11% of global deaths according to the WHO (World Health Organization). The goal of this project is to develop a predictive model that can be used as an aid to identify at-risk individuals. The data is downloaded from kaggle.com. The author, fedesoriano, states the source is confidential. 

The data set contains 5110 observations and 12 predictors including the binary outcome indicating stroke. The predictors also include an anonymous id, and information for gender, age, hypertension, heart disease, ever married, work type, residence type, average glucose level, body mass index, and smoking status. No other background information is given about the data. 

After a validation set has been partitioned from the data, the remaining data will be cleaned, explored, and used to build several models. The best model will be tested against the validation set, and the resulting scores will determine if the goal of this project succeeded. The main score that will be used to grade performance will be the **Precision**. Precision is defined as the proportion of predicted positive cases that are actually positive.

# Method

The data was downloaded as a zip file from the kaggle website, extracted into the working directory of this project, and read into R. Because the data set is relatively small, only 10% was set aside as the validation set. The remaining data is cleaned by setting the predictor values from character types to either numeric values or factors.
The bmi predictor has numerous NA values that will not work with the machine learning algorithms. For now these NA values are replaced with the average value for the sake of exploration.
```{r, include=F}
# Load
#######################################################################################################
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(ROSE)) install.packages("ROSE", repos = "http://cran.us.r-project.org")
# Load libraries
library(tidyverse)
library(caret) # Machine learning algos
library(scales) # Custom ggplot axis scales
library(ROSE) # Data balancing
# Data source: https://www.kaggle.com/fedesoriano/stroke-prediction-dataset
# Load the stroke data
healthcare_data <- read_csv('healthcare-dataset-stroke-data.csv')
# Extract Validation set
set.seed(420, sample.kind = 'Rounding')
test_index <- createDataPartition(healthcare_data$stroke, times = 1, p = 0.5, list = FALSE)
validation_set <- healthcare_data[test_index, ]
stroke <- healthcare_data[-test_index, ]
# Remove un-needed variables
rm(test_index, healthcare_data)
#######################################################################################################



# Munge
#######################################################################################################
# Smoking status as factor, & reorder to appear better in plots
stroke <- stroke %>% mutate(smoking_status=fct_relevel(as.factor(smoking_status), 
                                                       'smokes',
                                                       'formerly smoked',
                                                       'Unknown',
                                                       'never smoked'))
# Other variables as factors will help models later on
stroke <- stroke %>% mutate(gender=as.factor(gender),
                            hypertension=as.factor(hypertension),
                            heart_disease=as.factor(heart_disease),
                            ever_married=as.factor(ever_married),
                            work_type=as.factor(work_type),
                            Residence_type=as.factor(Residence_type),
                            stroke=as.factor(stroke),
                            bmi=as.numeric(bmi)) # bmi has NA's

# Replace NA values in BMI to mean
stroke$bmi[is.na(stroke$bmi)] <- mean(stroke$bmi, na.rm = TRUE)
#######################################################################################################

```
The first plot shows that the data contains more observations of females the proportional difference in strokes is not obvious however.
```{r, echo=F, message=F}
# Barplot of stroke & gender
stroke %>% group_by(stroke, gender) %>% summarise(n=n()) %>%
  ggplot(aes(x=stroke, fill=gender, y=n))+
  geom_col(position='dodge', color='black')+
  ylab('Count')+
  xlab('Stroke')+
  scale_fill_brewer(palette = "Pastel1")
```
This plot shows that the rate of strokes is about the same between genders.
```{r, echo=F, message=F}
# The dataset has a lot more females, and about equal amount of strokes
stroke %>% group_by(stroke, gender) %>% summarise(n=n()) %>%
  ggplot(aes(x=gender, fill=stroke, y=n))+
  geom_col(position='fill', color='black')+
  scale_y_continuous(label=scales::percent)+
  xlab('Stroke')+
  ylab('Proportion')+
  scale_fill_brewer(palette = "Dark2")
```
The next plot shows a cumulative distribution function of stroke patients for age and gender. Females have an exponential curve where the liklihood of a stroke increases rather smoothly. Men start off low and see a sharp increase in their mid to late 50's, overtaking the women until they even out in the early 70's.
```{r, echo=F, message=F}
# CDF of age for people with strokes
stroke[stroke$stroke == 1, ] %>% group_by(gender) %>%
  ggplot(aes(x=age, color=gender))+
  stat_ecdf(geom = 'step')+
  scale_y_continuous(label=scales::percent)+
  ylab('Strokes Occured')
```
The next plot shows a histogram of average glucose level. The distribution is bimodal with the higher glucose levels being less prevalent and much higher percentage of strokes.
```{r, echo=F, message=F}
stroke %>% group_by(stroke) %>%
  ggplot(aes(x=avg_glucose_level, color=stroke))+
  geom_histogram(bins=30)
```
The next bar plot shows a bar of stroke positive and a bar of stroke negative cases, filled in with smoking status. Non smokers appear equally likely to get a stroke. The same is true for patients who smoke their entire life. This graph suggests that quitting increases the chances of getting a stroke. 
```{r, echo=F, message=F}
# Smoking Status
stroke %>% group_by(smoking_status, stroke) %>% summarise(n=n()) %>%
  ggplot(aes(x=stroke, y=n, fill=smoking_status))+
  geom_col(position='fill', color='black')+
  scale_y_continuous(label=scales::percent)
```
It is not clear if the stroke cases quit smoking for a few days and promptly relapsed, or if the non stroke patients quit smoking after only several years of smoking. There is no way to account for length of smoking abstinence with the given data, however the next plot shows a CDF of strokes and age for each of the smoking categories. This plot suggests that smokers get strokes earlier than non smokers, and former smokers are clearly having strokes at older ages than people who did not quit.
```{r, echo=F, message=F}
stroke[stroke$stroke == 1, ] %>% group_by(smoking_status) %>%
     ggplot(aes(x=age, color=smoking_status))+
     stat_ecdf(geom = 'step')+
     scale_y_continuous(label=scales::percent)+
     ylab('Strokes Occured')
```
The next plot shows that hypertension is higher in stroke patients than non stroke patients.
```{r, echo=F, message=F}
# Hypertension (high blood pressure), heart disease, & high bmi
stroke %>% group_by(stroke, hypertension) %>% summarise(n=n()) %>%
  ggplot(aes(x=hypertension, y=n, color=stroke))+
  scale_y_continuous(label=scales::percent)+
  ylab('Proportion')+
  geom_col(position='fill')
```
The next plot trys to find a relationship between bmi and average glucose level. A relationship with bmi is wanted because there are so many NA values in the data. As it appears there is no strong relationship, and the average value is skewing the data, the bmi predictor must be dropped from the data before moving onto the model building.
```{r, echo=F, message=F}
# BMI
stroke %>% #mutate_if(is.numeric, ~replace(., is.na(.), 0)) %>% 
  arrange(stroke) %>%
  ggplot(aes(x=bmi, y=avg_glucose_level, color=stroke))+
  geom_point()
```
So far there is no single obvious predictor for identifying stokes. Instead, each predictor shows some small predictive power. Also as we have seen the data is skewed with very little data for stroke cases. 

Before building the first model it is necessary to to remove the bmi and id predictors. The NA values will cause errors in the machine learning algorithms and the ids  shouldn't have any predictive power. A training set and test set are then partitioned from the data, again at 9 to 1 ratio due to the size of the data.
```{r, include=F}
# Remove variables that negatively impact models
stroke <- stroke %>% select(-bmi, -id)

# Get a test/train set of data
set.seed(69, sample.kind = 'Rounding')
test_index <- createDataPartition(stroke$stroke, times = 1, p = 0.1, list = FALSE)
test <- stroke[test_index, ]
train <- stroke[-test_index, ]
rm(test_index)
```
The first model, a decision tree, is trained and tested giving the results below:
```{r, echo=F}
# Descision Tree
#----------------------------------------------------
train_tree <- train(stroke ~ .,
                    method='rpart',
                    data=train,
                    tuneGrid=data.frame(cp=seq(0, 0.005, len=25)))
ggplot(train_tree, highlight=T)
y_hat_tree <- predict(train_tree, test)

# Evaluate Tree Model
confusionMatrix(y_hat_tree, test$stroke)$overall["Accuracy"]
#accuracy.meas(test$stroke, y_hat_tree) #ROSE
roc.curve(test$stroke, y_hat_tree) #ROSE
```
It is obvious that the unbalanced nature of the data is effecting the model. To remedy this, the ROSE package will be used to try several approaches of data balancing. Over-sampling replicates random observations in the minority class until meeting a predefined ratio, this approach has a greater risk of overfitting the data. Under-sampling is a method of randomly removing observations in the majority class until meeting a predefined ratio, this approach loses a significant portion of the data. The ROSE package makes implementing both of these methods easy, and provides two additional methods. The first is a combination of the over-sampling and under-sampling methods, and the second creates synthetic data that is inserted into the minority class. The resulting dataset sizes and the ROC curves when using these methods is shown below.
```{r, echo=F, message=F}
# Data Balancing
#######################################################################################################
# Unbalanced
table(train$stroke)
prop.table(table(train$stroke))

# Over-sample
train.over <- ovun.sample(stroke ~ ., data=train, method='over', N=2183*2)$data
table(train.over$stroke)

# Under-sample
train.under <- ovun.sample(stroke ~ ., data=train, method='under', N=116*2)$data
table(train.under$stroke)

# Over & Under
train.both <- ovun.sample(stroke ~ ., data=train, method='both', p=0.5)$data
table(train.both$stroke)

# Inject Synthetic Data
train.rose <- ROSE(stroke ~ ., data=train, seed = 69)$data
table(train.rose$stroke)

# Select best data-balancing technique based on ROC of tree model
tree.over <- train(stroke ~ ., method='rpart', data=train.over, tuneGrid=data.frame(cp=seq(0, 0.005, len=25)))
tree.under <- train(stroke ~ ., method='rpart', data=train.under, tuneGrid=data.frame(cp=seq(0, 0.005, len=25)))
tree.both <- train(stroke ~ ., method='rpart', data=train.both, tuneGrid=data.frame(cp=seq(0, 0.005, len=25)))
tree.rose <- train(stroke ~ ., method='rpart', data=train.rose, tuneGrid=data.frame(cp=seq(0, 0.005, len=25)))
pred.tree.over <- predict(tree.over, test)
pred.tree.under <- predict(tree.under, test)
pred.tree.both <- predict(tree.both, test)
pred.tree.rose <- predict(tree.rose, test)
roc.curve(test$stroke, y_hat_tree, col=2, lwd=2)
roc.curve(test$stroke, pred.tree.over, add.roc = T, col=3, lwd=2)
roc.curve(test$stroke, pred.tree.under, add.roc = T, col=4, lwd=2)
roc.curve(test$stroke, pred.tree.both, add.roc = T, col=5, lwd=2)
roc.curve(test$stroke, pred.tree.rose, add.roc = T, col=6, lwd=2)
legend('bottomright', c('Not Balanced', 'Over', 'Under', 'Both', 'ROSE'), col=2:6, lwd=2)

```
The ROSE method of injecting synthetic data gives the best results. So this data is used with several other machine learning algorithms below.






# Citation

https://www.kaggle.com/fedesoriano/stroke-prediction-dataset